{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "887e5ca4",
   "metadata": {},
   "source": [
    "Project -Sentment Analysis On-IMDB Movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cc7df0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "350b0f28",
   "metadata": {},
   "source": [
    "# Importing all required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "814a3c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Import various modules for string cleaning\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "# Download the punkt tokenizer for sentence splitting\n",
    "import nltk.data\n",
    "#nltk.download()   \n",
    "from gensim.models import word2vec\n",
    "#word2vec tries to learn relationships between words and embeds them in a lower-dimensional vector space\n",
    "#The Word2Vec model is used to extract the notion of relatedness across words or products such as semantic relatedness, synonym detection, concept categorization, selectional preferences, and analogy. A Word2Vec model learns meaningful relations and encodes the relatedness into vector similarity\n",
    "#Word2vec is a neural network structure to generate word embedding by training the model on a supervised classification problem\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35fdcc6",
   "metadata": {},
   "source": [
    "# Importing data using read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edcf68d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train=pd.read_csv(\"labeledTraindata.tsv\",header=0, delimiter=\"\\t\", quoting=3)\n",
    "test=pd.read_csv(\"testData.tsv\", header=0, delimiter=\"\\t\",  quoting=3)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7afcf582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b9a0a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['id', 'sentiment', 'review'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5430ffab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ab276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0011e5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 25000 test reviews, \n"
     ]
    }
   ],
   "source": [
    "print(\"Read %d labeled train reviews, %d test reviews, \" % (train[\"review\"].size,test[\"review\"].size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df659f67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f791a1d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b2e114a",
   "metadata": {},
   "source": [
    "# Function to clean the data, using beautiful soup to clean HTML tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8682cb",
   "metadata": {},
   "source": [
    "When we do data scrping from websites we recieve html tags, HTML language helps design and develop maintain websites. can use regular expression but beautiful soup is faster and less time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eab0217a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_wordlist( review, remove_stopwords=False ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    #  \n",
    "    # 2. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfc02bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\dell\\anaconda3\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from gensim) (6.0.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from gensim) (1.20.3)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from gensim) (1.7.1)\n",
      "Requirement already satisfied: Cython==0.29.28 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from gensim) (0.29.28)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c9ff572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c684e17",
   "metadata": {},
   "source": [
    "# Creating lists with tokenized sentence where each list is a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "709bb62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( review_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords ))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f9f5b9",
   "metadata": {},
   "source": [
    "# Running the functions to clean the data and create a corpus of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ae974cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:337: MarkupResemblesLocatorWarning: \".\" looks like a directory name, not markup. You may want to open a file found in this directory and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:337: MarkupResemblesLocatorWarning: \"...\" looks like a directory name, not markup. You may want to open a file found in this directory and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:431: MarkupResemblesLocatorWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from unlabeled set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:337: MarkupResemblesLocatorWarning: \"..\" looks like a directory name, not markup. You may want to open a file found in this directory and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:337: MarkupResemblesLocatorWarning: \"....................................................................\" looks like a directory name, not markup. You may want to open a file found in this directory and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print(\"Parsing sentences from training set\")\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "\n",
    "print(\"Parsing sentences from unlabeled set\")\n",
    "for review in test[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8da1e48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "527951\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "957e0727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'started', 'listening', 'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again']\n"
     ]
    }
   ],
   "source": [
    "print (sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a85a63a",
   "metadata": {},
   "source": [
    "bold text- sentences (iterable of iterables, optional) – The sentences iterable can be simply a list of lists of tokens, but for larger corpora, consider an iterable that streams the sentences directly from disk/network. See BrownCorpus, Text8Corpus or LineSentence in word2vec module for such examples. See also the tutorial on data streaming in Python. If you don’t supply sentences, the model is left uninitialized – use if you plan to initialize it in some other way.\n",
    "\n",
    "corpus_file (str, optional) – Path to a corpus file in LineSentence format. You may use this argument instead of sentences to get performance boost. Only one of sentences or corpus_file arguments need to be passed (or none of them, in that case, the model is left uninitialized).\n",
    "vector_size (int, optional) – Dimensionality of the word vectors.\n",
    "\n",
    "window (int, optional) – Maximum distance between the current and predicted word within a sentence.\n",
    "\n",
    "min_count (int, optional) – Ignores all words with total frequency lower than this.\n",
    "\n",
    "workers (int, optional) – Use these many worker threads to train the model (=faster training with multicore machines).\n",
    "\n",
    "sample (float, optional) – The threshold for configuring which higher-frequency words are randomly downsampled, useful range is (0, 1e-5)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eacc73",
   "metadata": {},
   "source": [
    "### Creating the word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d42893a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8beaeb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-02 16:44:25,120 : INFO : collecting all words and their counts\n",
      "2022-06-02 16:44:25,123 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2022-06-02 16:44:25,234 : INFO : PROGRESS: at sentence #10000, processed 225803 words, keeping 17776 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-02 16:44:25,337 : INFO : PROGRESS: at sentence #20000, processed 451892 words, keeping 24948 word types\n",
      "2022-06-02 16:44:25,459 : INFO : PROGRESS: at sentence #30000, processed 671315 words, keeping 30034 word types\n",
      "2022-06-02 16:44:25,572 : INFO : PROGRESS: at sentence #40000, processed 897815 words, keeping 34348 word types\n",
      "2022-06-02 16:44:25,677 : INFO : PROGRESS: at sentence #50000, processed 1116963 words, keeping 37761 word types\n",
      "2022-06-02 16:44:25,780 : INFO : PROGRESS: at sentence #60000, processed 1338404 words, keeping 40723 word types\n",
      "2022-06-02 16:44:25,883 : INFO : PROGRESS: at sentence #70000, processed 1561580 words, keeping 43333 word types\n",
      "2022-06-02 16:44:25,987 : INFO : PROGRESS: at sentence #80000, processed 1780887 words, keeping 45714 word types\n",
      "2022-06-02 16:44:26,090 : INFO : PROGRESS: at sentence #90000, processed 2004996 words, keeping 48135 word types\n",
      "2022-06-02 16:44:26,196 : INFO : PROGRESS: at sentence #100000, processed 2226966 words, keeping 50207 word types\n",
      "2022-06-02 16:44:26,298 : INFO : PROGRESS: at sentence #110000, processed 2446580 words, keeping 52081 word types\n",
      "2022-06-02 16:44:26,411 : INFO : PROGRESS: at sentence #120000, processed 2668775 words, keeping 54119 word types\n",
      "2022-06-02 16:44:26,513 : INFO : PROGRESS: at sentence #130000, processed 2894303 words, keeping 55847 word types\n",
      "2022-06-02 16:44:26,620 : INFO : PROGRESS: at sentence #140000, processed 3107005 words, keeping 57346 word types\n",
      "2022-06-02 16:44:26,726 : INFO : PROGRESS: at sentence #150000, processed 3332627 words, keeping 59055 word types\n",
      "2022-06-02 16:44:26,855 : INFO : PROGRESS: at sentence #160000, processed 3555315 words, keeping 60617 word types\n",
      "2022-06-02 16:44:26,982 : INFO : PROGRESS: at sentence #170000, processed 3778655 words, keeping 62077 word types\n",
      "2022-06-02 16:44:27,100 : INFO : PROGRESS: at sentence #180000, processed 3999236 words, keeping 63496 word types\n",
      "2022-06-02 16:44:27,205 : INFO : PROGRESS: at sentence #190000, processed 4224449 words, keeping 64794 word types\n",
      "2022-06-02 16:44:27,309 : INFO : PROGRESS: at sentence #200000, processed 4448603 words, keeping 66087 word types\n",
      "2022-06-02 16:44:27,419 : INFO : PROGRESS: at sentence #210000, processed 4669967 words, keeping 67390 word types\n",
      "2022-06-02 16:44:27,568 : INFO : PROGRESS: at sentence #220000, processed 4894968 words, keeping 68697 word types\n",
      "2022-06-02 16:44:27,673 : INFO : PROGRESS: at sentence #230000, processed 5117545 words, keeping 69958 word types\n",
      "2022-06-02 16:44:27,776 : INFO : PROGRESS: at sentence #240000, processed 5345050 words, keeping 71167 word types\n",
      "2022-06-02 16:44:27,903 : INFO : PROGRESS: at sentence #250000, processed 5559165 words, keeping 72351 word types\n",
      "2022-06-02 16:44:28,022 : INFO : PROGRESS: at sentence #260000, processed 5779146 words, keeping 73478 word types\n",
      "2022-06-02 16:44:28,136 : INFO : PROGRESS: at sentence #270000, processed 5997333 words, keeping 74782 word types\n",
      "2022-06-02 16:44:28,253 : INFO : PROGRESS: at sentence #280000, processed 6224877 words, keeping 76376 word types\n",
      "2022-06-02 16:44:28,361 : INFO : PROGRESS: at sentence #290000, processed 6444211 words, keeping 77868 word types\n",
      "2022-06-02 16:44:28,468 : INFO : PROGRESS: at sentence #300000, processed 6665775 words, keeping 79317 word types\n",
      "2022-06-02 16:44:28,579 : INFO : PROGRESS: at sentence #310000, processed 6885907 words, keeping 80540 word types\n",
      "2022-06-02 16:44:28,713 : INFO : PROGRESS: at sentence #320000, processed 7108401 words, keeping 81739 word types\n",
      "2022-06-02 16:44:28,876 : INFO : PROGRESS: at sentence #330000, processed 7333331 words, keeping 83037 word types\n",
      "2022-06-02 16:44:29,025 : INFO : PROGRESS: at sentence #340000, processed 7553937 words, keeping 84196 word types\n",
      "2022-06-02 16:44:29,167 : INFO : PROGRESS: at sentence #350000, processed 7769863 words, keeping 85206 word types\n",
      "2022-06-02 16:44:29,273 : INFO : PROGRESS: at sentence #360000, processed 7992103 words, keeping 86353 word types\n",
      "2022-06-02 16:44:29,377 : INFO : PROGRESS: at sentence #370000, processed 8208190 words, keeping 87291 word types\n",
      "2022-06-02 16:44:29,488 : INFO : PROGRESS: at sentence #380000, processed 8428594 words, keeping 88185 word types\n",
      "2022-06-02 16:44:29,600 : INFO : PROGRESS: at sentence #390000, processed 8654326 words, keeping 89159 word types\n",
      "2022-06-02 16:44:29,721 : INFO : PROGRESS: at sentence #400000, processed 8875171 words, keeping 90164 word types\n",
      "2022-06-02 16:44:29,828 : INFO : PROGRESS: at sentence #410000, processed 9096929 words, keeping 91090 word types\n",
      "2022-06-02 16:44:29,962 : INFO : PROGRESS: at sentence #420000, processed 9319824 words, keeping 92084 word types\n",
      "2022-06-02 16:44:30,072 : INFO : PROGRESS: at sentence #430000, processed 9542760 words, keeping 93082 word types\n",
      "2022-06-02 16:44:30,210 : INFO : PROGRESS: at sentence #440000, processed 9763519 words, keeping 93989 word types\n",
      "2022-06-02 16:44:30,367 : INFO : PROGRESS: at sentence #450000, processed 9979011 words, keeping 94819 word types\n",
      "2022-06-02 16:44:30,530 : INFO : PROGRESS: at sentence #460000, processed 10202053 words, keeping 95773 word types\n",
      "2022-06-02 16:44:30,667 : INFO : PROGRESS: at sentence #470000, processed 10423212 words, keeping 96650 word types\n",
      "2022-06-02 16:44:30,775 : INFO : PROGRESS: at sentence #480000, processed 10638873 words, keeping 97494 word types\n",
      "2022-06-02 16:44:30,892 : INFO : PROGRESS: at sentence #490000, processed 10863435 words, keeping 98397 word types\n",
      "2022-06-02 16:44:30,993 : INFO : PROGRESS: at sentence #500000, processed 11084004 words, keeping 99159 word types\n",
      "2022-06-02 16:44:31,108 : INFO : PROGRESS: at sentence #510000, processed 11305838 words, keeping 100006 word types\n",
      "2022-06-02 16:44:31,247 : INFO : PROGRESS: at sentence #520000, processed 11526766 words, keeping 100801 word types\n",
      "2022-06-02 16:44:31,342 : INFO : collected 101399 word types from a corpus of 11707067 raw words and 527951 sentences\n",
      "2022-06-02 16:44:31,344 : INFO : Creating a fresh vocabulary\n",
      "2022-06-02 16:44:31,560 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=40 retains 12857 unique words (12.68% of original 101399, drops 88542)', 'datetime': '2022-06-02T16:44:31.560985', 'gensim': '4.2.0', 'python': '3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19043-SP0', 'event': 'prepare_vocab'}\n",
      "2022-06-02 16:44:31,561 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=40 leaves 11232399 word corpus (95.95% of original 11707067, drops 474668)', 'datetime': '2022-06-02T16:44:31.561985', 'gensim': '4.2.0', 'python': '3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19043-SP0', 'event': 'prepare_vocab'}\n",
      "2022-06-02 16:44:31,922 : INFO : deleting the raw counts dictionary of 101399 items\n",
      "2022-06-02 16:44:31,927 : INFO : sample=0.001 downsamples 49 most-common words\n",
      "2022-06-02 16:44:31,933 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 8259240.433887313 word corpus (73.5%% of prior 11232399)', 'datetime': '2022-06-02T16:44:31.933486', 'gensim': '4.2.0', 'python': '3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19043-SP0', 'event': 'prepare_vocab'}\n",
      "2022-06-02 16:44:32,365 : INFO : estimated required memory for 12857 words and 300 dimensions: 37285300 bytes\n",
      "2022-06-02 16:44:32,366 : INFO : resetting layer weights\n",
      "2022-06-02 16:44:32,438 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-06-02T16:44:32.438599', 'gensim': '4.2.0', 'python': '3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19043-SP0', 'event': 'build_vocab'}\n",
      "2022-06-02 16:44:32,439 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 12857 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2022-06-02T16:44:32.439594', 'gensim': '4.2.0', 'python': '3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19043-SP0', 'event': 'train'}\n",
      "2022-06-02 16:44:33,521 : INFO : EPOCH 0 - PROGRESS: at 1.59% examples, 130140 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-02 16:44:34,526 : INFO : EPOCH 0 - PROGRESS: at 4.99% examples, 204136 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:44:35,593 : INFO : EPOCH 0 - PROGRESS: at 8.18% examples, 220003 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:44:36,615 : INFO : EPOCH 0 - PROGRESS: at 11.54% examples, 232208 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:44:37,687 : INFO : EPOCH 0 - PROGRESS: at 14.08% examples, 224953 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:44:38,711 : INFO : EPOCH 0 - PROGRESS: at 16.97% examples, 226452 words/s, in_qsize 8, out_qsize 0\n",
      "2022-06-02 16:44:39,745 : INFO : EPOCH 0 - PROGRESS: at 19.72% examples, 225362 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:44:40,756 : INFO : EPOCH 0 - PROGRESS: at 22.79% examples, 228473 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:44:41,785 : INFO : EPOCH 0 - PROGRESS: at 26.23% examples, 233502 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:44:42,837 : INFO : EPOCH 0 - PROGRESS: at 29.86% examples, 238998 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:44:43,853 : INFO : EPOCH 0 - PROGRESS: at 33.45% examples, 243700 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:44:44,875 : INFO : EPOCH 0 - PROGRESS: at 36.92% examples, 246936 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:44:45,906 : INFO : EPOCH 0 - PROGRESS: at 38.96% examples, 240548 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:44:46,906 : INFO : EPOCH 0 - PROGRESS: at 41.56% examples, 238980 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:44:47,916 : INFO : EPOCH 0 - PROGRESS: at 44.93% examples, 241578 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:44:49,018 : INFO : EPOCH 0 - PROGRESS: at 47.56% examples, 238244 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:44:50,021 : INFO : EPOCH 0 - PROGRESS: at 50.30% examples, 237472 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:44:51,021 : INFO : EPOCH 0 - PROGRESS: at 53.43% examples, 238690 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:44:52,023 : INFO : EPOCH 0 - PROGRESS: at 56.41% examples, 239053 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:44:53,029 : INFO : EPOCH 0 - PROGRESS: at 59.43% examples, 239348 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:44:54,072 : INFO : EPOCH 0 - PROGRESS: at 62.03% examples, 237909 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:44:55,087 : INFO : EPOCH 0 - PROGRESS: at 64.17% examples, 235005 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:44:56,208 : INFO : EPOCH 0 - PROGRESS: at 66.78% examples, 232830 words/s, in_qsize 8, out_qsize 0\n",
      "2022-06-02 16:44:57,209 : INFO : EPOCH 0 - PROGRESS: at 69.98% examples, 233968 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:44:58,216 : INFO : EPOCH 0 - PROGRESS: at 73.13% examples, 234939 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:44:59,220 : INFO : EPOCH 0 - PROGRESS: at 74.71% examples, 231124 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:45:00,228 : INFO : EPOCH 0 - PROGRESS: at 77.72% examples, 231609 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:45:01,265 : INFO : EPOCH 0 - PROGRESS: at 80.85% examples, 232318 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:45:02,277 : INFO : EPOCH 0 - PROGRESS: at 83.93% examples, 232918 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:45:03,294 : INFO : EPOCH 0 - PROGRESS: at 86.96% examples, 233211 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:45:04,324 : INFO : EPOCH 0 - PROGRESS: at 90.26% examples, 234087 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:45:05,379 : INFO : EPOCH 0 - PROGRESS: at 93.12% examples, 233863 words/s, in_qsize 6, out_qsize 1\n",
      "2022-06-02 16:45:06,422 : INFO : EPOCH 0 - PROGRESS: at 96.21% examples, 234124 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:45:07,459 : INFO : EPOCH 0 - PROGRESS: at 99.37% examples, 234629 words/s, in_qsize 6, out_qsize 1\n",
      "2022-06-02 16:45:07,635 : INFO : EPOCH 0: training on 11707067 raw words (8259368 effective words) took 35.1s, 235018 effective words/s\n",
      "2022-06-02 16:45:08,659 : INFO : EPOCH 1 - PROGRESS: at 2.95% examples, 244437 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:45:09,669 : INFO : EPOCH 1 - PROGRESS: at 4.73% examples, 195399 words/s, in_qsize 8, out_qsize 0\n",
      "2022-06-02 16:45:10,671 : INFO : EPOCH 1 - PROGRESS: at 7.25% examples, 200578 words/s, in_qsize 8, out_qsize 1\n",
      "2022-06-02 16:45:11,678 : INFO : EPOCH 1 - PROGRESS: at 10.24% examples, 211434 words/s, in_qsize 8, out_qsize 0\n",
      "2022-06-02 16:45:12,693 : INFO : EPOCH 1 - PROGRESS: at 13.23% examples, 217827 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:45:13,699 : INFO : EPOCH 1 - PROGRESS: at 16.37% examples, 224666 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:45:14,700 : INFO : EPOCH 1 - PROGRESS: at 18.26% examples, 214762 words/s, in_qsize 8, out_qsize 0\n",
      "2022-06-02 16:45:15,712 : INFO : EPOCH 1 - PROGRESS: at 20.74% examples, 213225 words/s, in_qsize 6, out_qsize 1\n",
      "2022-06-02 16:45:16,809 : INFO : EPOCH 1 - PROGRESS: at 23.87% examples, 216175 words/s, in_qsize 8, out_qsize 0\n",
      "2022-06-02 16:45:17,846 : INFO : EPOCH 1 - PROGRESS: at 27.16% examples, 220379 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:45:18,891 : INFO : EPOCH 1 - PROGRESS: at 30.46% examples, 224363 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:45:19,941 : INFO : EPOCH 1 - PROGRESS: at 33.79% examples, 227569 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:45:20,978 : INFO : EPOCH 1 - PROGRESS: at 37.25% examples, 231555 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:45:22,052 : INFO : EPOCH 1 - PROGRESS: at 39.87% examples, 229444 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:45:23,071 : INFO : EPOCH 1 - PROGRESS: at 43.01% examples, 231156 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:45:24,217 : INFO : EPOCH 1 - PROGRESS: at 46.58% examples, 233014 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:45:25,261 : INFO : EPOCH 1 - PROGRESS: at 49.08% examples, 230771 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:45:26,266 : INFO : EPOCH 1 - PROGRESS: at 52.56% examples, 233807 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:45:27,285 : INFO : EPOCH 1 - PROGRESS: at 55.06% examples, 232090 words/s, in_qsize 8, out_qsize 0\n",
      "2022-06-02 16:45:28,310 : INFO : EPOCH 1 - PROGRESS: at 58.41% examples, 233855 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:45:29,347 : INFO : EPOCH 1 - PROGRESS: at 61.77% examples, 235665 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:45:30,388 : INFO : EPOCH 1 - PROGRESS: at 65.47% examples, 238230 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:45:31,449 : INFO : EPOCH 1 - PROGRESS: at 68.40% examples, 237674 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:45:32,498 : INFO : EPOCH 1 - PROGRESS: at 71.27% examples, 236999 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:45:33,529 : INFO : EPOCH 1 - PROGRESS: at 73.71% examples, 235450 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:45:34,548 : INFO : EPOCH 1 - PROGRESS: at 76.45% examples, 234906 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:45:35,573 : INFO : EPOCH 1 - PROGRESS: at 79.83% examples, 236388 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:45:36,639 : INFO : EPOCH 1 - PROGRESS: at 83.41% examples, 237874 words/s, in_qsize 6, out_qsize 1\n",
      "2022-06-02 16:45:37,644 : INFO : EPOCH 1 - PROGRESS: at 86.96% examples, 239518 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:45:38,654 : INFO : EPOCH 1 - PROGRESS: at 90.34% examples, 240576 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:45:39,683 : INFO : EPOCH 1 - PROGRESS: at 93.56% examples, 241212 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:45:40,684 : INFO : EPOCH 1 - PROGRESS: at 96.12% examples, 240283 words/s, in_qsize 8, out_qsize 1\n",
      "2022-06-02 16:45:41,698 : INFO : EPOCH 1 - PROGRESS: at 99.45% examples, 241194 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:45:41,835 : INFO : EPOCH 1: training on 11707067 raw words (8259779 effective words) took 34.2s, 241635 effective words/s\n",
      "2022-06-02 16:45:42,889 : INFO : EPOCH 2 - PROGRESS: at 1.84% examples, 151417 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:45:43,977 : INFO : EPOCH 2 - PROGRESS: at 4.81% examples, 190025 words/s, in_qsize 8, out_qsize 3\n",
      "2022-06-02 16:45:45,005 : INFO : EPOCH 2 - PROGRESS: at 6.92% examples, 183987 words/s, in_qsize 8, out_qsize 1\n",
      "2022-06-02 16:45:46,005 : INFO : EPOCH 2 - PROGRESS: at 10.24% examples, 205812 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:45:47,026 : INFO : EPOCH 2 - PROGRESS: at 13.65% examples, 219764 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-02 16:45:48,029 : INFO : EPOCH 2 - PROGRESS: at 15.62% examples, 210315 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:45:49,062 : INFO : EPOCH 2 - PROGRESS: at 18.52% examples, 213448 words/s, in_qsize 8, out_qsize 1\n",
      "2022-06-02 16:45:50,071 : INFO : EPOCH 2 - PROGRESS: at 21.43% examples, 216481 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:45:51,114 : INFO : EPOCH 2 - PROGRESS: at 24.63% examples, 221008 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:45:52,139 : INFO : EPOCH 2 - PROGRESS: at 27.08% examples, 218076 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:45:53,149 : INFO : EPOCH 2 - PROGRESS: at 29.54% examples, 216666 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:45:54,152 : INFO : EPOCH 2 - PROGRESS: at 32.33% examples, 217917 words/s, in_qsize 8, out_qsize 0\n",
      "2022-06-02 16:45:55,162 : INFO : EPOCH 2 - PROGRESS: at 35.31% examples, 219925 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:45:56,179 : INFO : EPOCH 2 - PROGRESS: at 38.44% examples, 222475 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:45:57,192 : INFO : EPOCH 2 - PROGRESS: at 41.07% examples, 221981 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:45:58,223 : INFO : EPOCH 2 - PROGRESS: at 44.00% examples, 223071 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:45:59,285 : INFO : EPOCH 2 - PROGRESS: at 47.01% examples, 223578 words/s, in_qsize 8, out_qsize 0\n",
      "2022-06-02 16:46:00,292 : INFO : EPOCH 2 - PROGRESS: at 49.87% examples, 223963 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:01,417 : INFO : EPOCH 2 - PROGRESS: at 52.22% examples, 221142 words/s, in_qsize 6, out_qsize 1\n",
      "2022-06-02 16:46:02,434 : INFO : EPOCH 2 - PROGRESS: at 55.48% examples, 223216 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:03,445 : INFO : EPOCH 2 - PROGRESS: at 58.58% examples, 224501 words/s, in_qsize 8, out_qsize 0\n",
      "2022-06-02 16:46:04,454 : INFO : EPOCH 2 - PROGRESS: at 60.60% examples, 221968 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:05,485 : INFO : EPOCH 2 - PROGRESS: at 63.56% examples, 222705 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:06,519 : INFO : EPOCH 2 - PROGRESS: at 66.94% examples, 224521 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:07,531 : INFO : EPOCH 2 - PROGRESS: at 70.24% examples, 226103 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:08,539 : INFO : EPOCH 2 - PROGRESS: at 72.87% examples, 225733 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:09,549 : INFO : EPOCH 2 - PROGRESS: at 76.00% examples, 226921 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:10,600 : INFO : EPOCH 2 - PROGRESS: at 78.66% examples, 226231 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:11,620 : INFO : EPOCH 2 - PROGRESS: at 81.80% examples, 227227 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:12,654 : INFO : EPOCH 2 - PROGRESS: at 84.99% examples, 228069 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:13,695 : INFO : EPOCH 2 - PROGRESS: at 87.96% examples, 228346 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:14,740 : INFO : EPOCH 2 - PROGRESS: at 91.26% examples, 229221 words/s, in_qsize 8, out_qsize 0\n",
      "2022-06-02 16:46:15,747 : INFO : EPOCH 2 - PROGRESS: at 94.41% examples, 230107 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:16,783 : INFO : EPOCH 2 - PROGRESS: at 97.65% examples, 230939 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:17,443 : INFO : EPOCH 2: training on 11707067 raw words (8259446 effective words) took 35.6s, 232149 effective words/s\n",
      "2022-06-02 16:46:18,464 : INFO : EPOCH 3 - PROGRESS: at 1.68% examples, 141067 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:19,478 : INFO : EPOCH 3 - PROGRESS: at 4.03% examples, 167992 words/s, in_qsize 8, out_qsize 0\n",
      "2022-06-02 16:46:20,483 : INFO : EPOCH 3 - PROGRESS: at 6.83% examples, 189045 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:21,484 : INFO : EPOCH 3 - PROGRESS: at 9.90% examples, 204869 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:22,511 : INFO : EPOCH 3 - PROGRESS: at 12.06% examples, 198123 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:23,536 : INFO : EPOCH 3 - PROGRESS: at 14.59% examples, 199468 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:24,551 : INFO : EPOCH 3 - PROGRESS: at 17.40% examples, 203655 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:25,562 : INFO : EPOCH 3 - PROGRESS: at 20.32% examples, 207921 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:26,564 : INFO : EPOCH 3 - PROGRESS: at 23.36% examples, 212862 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:27,578 : INFO : EPOCH 3 - PROGRESS: at 26.41% examples, 215952 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:28,613 : INFO : EPOCH 3 - PROGRESS: at 29.54% examples, 219235 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:29,646 : INFO : EPOCH 3 - PROGRESS: at 32.42% examples, 220333 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:30,654 : INFO : EPOCH 3 - PROGRESS: at 35.55% examples, 223285 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:31,686 : INFO : EPOCH 3 - PROGRESS: at 38.44% examples, 223894 words/s, in_qsize 8, out_qsize 0\n",
      "2022-06-02 16:46:32,703 : INFO : EPOCH 3 - PROGRESS: at 41.63% examples, 226507 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:33,769 : INFO : EPOCH 3 - PROGRESS: at 43.84% examples, 222929 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:34,770 : INFO : EPOCH 3 - PROGRESS: at 46.32% examples, 221828 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:35,861 : INFO : EPOCH 3 - PROGRESS: at 48.73% examples, 219367 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:36,872 : INFO : EPOCH 3 - PROGRESS: at 52.06% examples, 222077 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:38,196 : INFO : EPOCH 3 - PROGRESS: at 55.23% examples, 220451 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:39,207 : INFO : EPOCH 3 - PROGRESS: at 57.72% examples, 219579 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:40,228 : INFO : EPOCH 3 - PROGRESS: at 60.77% examples, 220887 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:41,229 : INFO : EPOCH 3 - PROGRESS: at 63.92% examples, 222526 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:42,251 : INFO : EPOCH 3 - PROGRESS: at 66.86% examples, 223043 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:43,252 : INFO : EPOCH 3 - PROGRESS: at 69.81% examples, 223690 words/s, in_qsize 8, out_qsize 0\n",
      "2022-06-02 16:46:44,341 : INFO : EPOCH 3 - PROGRESS: at 73.04% examples, 224569 words/s, in_qsize 8, out_qsize 1\n",
      "2022-06-02 16:46:45,342 : INFO : EPOCH 3 - PROGRESS: at 74.98% examples, 222314 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:46,370 : INFO : EPOCH 3 - PROGRESS: at 77.45% examples, 221481 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:47,397 : INFO : EPOCH 3 - PROGRESS: at 79.92% examples, 220719 words/s, in_qsize 8, out_qsize 0\n",
      "2022-06-02 16:46:48,400 : INFO : EPOCH 3 - PROGRESS: at 82.64% examples, 220829 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:49,489 : INFO : EPOCH 3 - PROGRESS: at 84.99% examples, 219259 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:50,506 : INFO : EPOCH 3 - PROGRESS: at 87.38% examples, 218458 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:51,520 : INFO : EPOCH 3 - PROGRESS: at 90.59% examples, 219614 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:52,555 : INFO : EPOCH 3 - PROGRESS: at 93.73% examples, 220562 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:53,571 : INFO : EPOCH 3 - PROGRESS: at 96.54% examples, 220792 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:54,576 : INFO : EPOCH 3 - PROGRESS: at 99.60% examples, 221616 words/s, in_qsize 5, out_qsize 0\n",
      "2022-06-02 16:46:54,749 : INFO : EPOCH 3: training on 11707067 raw words (8258789 effective words) took 37.3s, 221501 effective words/s\n",
      "2022-06-02 16:46:55,869 : INFO : EPOCH 4 - PROGRESS: at 3.12% examples, 259183 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:56,876 : INFO : EPOCH 4 - PROGRESS: at 5.92% examples, 245164 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:57,903 : INFO : EPOCH 4 - PROGRESS: at 8.84% examples, 243391 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:58,905 : INFO : EPOCH 4 - PROGRESS: at 12.23% examples, 250955 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:46:59,909 : INFO : EPOCH 4 - PROGRESS: at 15.37% examples, 252716 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:47:00,931 : INFO : EPOCH 4 - PROGRESS: at 18.27% examples, 249646 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-02 16:47:01,982 : INFO : EPOCH 4 - PROGRESS: at 21.10% examples, 245511 words/s, in_qsize 8, out_qsize 0\n",
      "2022-06-02 16:47:02,989 : INFO : EPOCH 4 - PROGRESS: at 23.87% examples, 243716 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:47:04,000 : INFO : EPOCH 4 - PROGRESS: at 27.24% examples, 246759 words/s, in_qsize 5, out_qsize 2\n",
      "2022-06-02 16:47:05,031 : INFO : EPOCH 4 - PROGRESS: at 30.38% examples, 247398 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:47:06,081 : INFO : EPOCH 4 - PROGRESS: at 32.95% examples, 243106 words/s, in_qsize 8, out_qsize 0\n",
      "2022-06-02 16:47:07,089 : INFO : EPOCH 4 - PROGRESS: at 35.48% examples, 240334 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:47:08,100 : INFO : EPOCH 4 - PROGRESS: at 38.71% examples, 242230 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:47:09,161 : INFO : EPOCH 4 - PROGRESS: at 41.81% examples, 242452 words/s, in_qsize 8, out_qsize 0\n",
      "2022-06-02 16:47:10,173 : INFO : EPOCH 4 - PROGRESS: at 44.75% examples, 242526 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:47:11,177 : INFO : EPOCH 4 - PROGRESS: at 48.33% examples, 245295 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:47:12,219 : INFO : EPOCH 4 - PROGRESS: at 51.90% examples, 247599 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:47:13,229 : INFO : EPOCH 4 - PROGRESS: at 55.56% examples, 250462 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:47:14,256 : INFO : EPOCH 4 - PROGRESS: at 59.09% examples, 252063 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:47:15,312 : INFO : EPOCH 4 - PROGRESS: at 62.38% examples, 252479 words/s, in_qsize 5, out_qsize 2\n",
      "2022-06-02 16:47:16,322 : INFO : EPOCH 4 - PROGRESS: at 65.63% examples, 253098 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:47:17,327 : INFO : EPOCH 4 - PROGRESS: at 68.94% examples, 253693 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:47:18,390 : INFO : EPOCH 4 - PROGRESS: at 72.02% examples, 253009 words/s, in_qsize 8, out_qsize 0\n",
      "2022-06-02 16:47:19,399 : INFO : EPOCH 4 - PROGRESS: at 75.41% examples, 254085 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:47:20,429 : INFO : EPOCH 4 - PROGRESS: at 78.31% examples, 253227 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:47:21,557 : INFO : EPOCH 4 - PROGRESS: at 80.59% examples, 249669 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:47:22,560 : INFO : EPOCH 4 - PROGRESS: at 83.92% examples, 250518 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:47:23,567 : INFO : EPOCH 4 - PROGRESS: at 86.68% examples, 249551 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:47:24,608 : INFO : EPOCH 4 - PROGRESS: at 89.44% examples, 248411 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:47:25,640 : INFO : EPOCH 4 - PROGRESS: at 92.69% examples, 248787 words/s, in_qsize 6, out_qsize 1\n",
      "2022-06-02 16:47:26,640 : INFO : EPOCH 4 - PROGRESS: at 94.59% examples, 245822 words/s, in_qsize 7, out_qsize 0\n",
      "2022-06-02 16:47:27,680 : INFO : EPOCH 4 - PROGRESS: at 97.38% examples, 245127 words/s, in_qsize 8, out_qsize 0\n",
      "2022-06-02 16:47:28,460 : INFO : EPOCH 4: training on 11707067 raw words (8260917 effective words) took 33.6s, 245875 effective words/s\n",
      "2022-06-02 16:47:28,462 : INFO : Word2Vec lifecycle event {'msg': 'training on 58535335 raw words (41298299 effective words) took 176.0s, 234620 effective words/s', 'datetime': '2022-06-02T16:47:28.462648', 'gensim': '4.2.0', 'python': '3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19043-SP0', 'event': 'train'}\n",
      "2022-06-02 16:47:28,465 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=12857, vector_size=300, alpha=0.025>', 'datetime': '2022-06-02T16:47:28.465646', 'gensim': '4.2.0', 'python': '3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19043-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers,vector_size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44a7f0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp/ipykernel_1072/3438567916.py:3: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  model.init_sims(replace=True)\n",
      "2022-06-02 16:47:29,782 : WARNING : destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n"
     ]
    }
   ],
   "source": [
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9fadd079",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-02 16:47:30,193 : INFO : Word2Vec lifecycle event {'fname_or_handle': '300features_40minwords_10context', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-06-02T16:47:30.193584', 'gensim': '4.2.0', 'python': '3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19043-SP0', 'event': 'saving'}\n",
      "2022-06-02 16:47:30,197 : INFO : not storing attribute cum_table\n",
      "2022-06-02 16:47:30,295 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65333cde",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13e8fad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6aa7e33d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'berlin'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(\"france england germany berlin\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba39e579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('jacket', 0.5657969117164612),\n",
       " ('sky', 0.5363483428955078),\n",
       " ('circle', 0.53548264503479),\n",
       " ('sun', 0.5287894010543823),\n",
       " ('oz', 0.5033456087112427),\n",
       " ('snow', 0.4892703890800476),\n",
       " ('snowy', 0.48059624433517456),\n",
       " ('spaceship', 0.4780445098876953),\n",
       " ('mountain', 0.4759177565574646),\n",
       " ('apollo', 0.4758463501930237)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"moon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b955bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('princess', 0.7508115768432617),\n",
       " ('bride', 0.6475000381469727),\n",
       " ('femme', 0.6413792371749878),\n",
       " ('nurse', 0.6368417739868164),\n",
       " ('goddess', 0.6298828125),\n",
       " ('gypsy', 0.62645423412323),\n",
       " ('fatale', 0.6238094568252563),\n",
       " ('sultry', 0.6188187003135681),\n",
       " ('belle', 0.6071270704269409),\n",
       " ('carmen', 0.6012665033340454)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d25d78a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'moon'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(\"dog cat elephant moon\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "656d4186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('suspense', 0.5038391947746277),\n",
       " ('pacing', 0.45792388916015625),\n",
       " ('paced', 0.4573788642883301),\n",
       " ('exciting', 0.4536362886428833),\n",
       " ('adventure', 0.4525851607322693),\n",
       " ('tension', 0.43860137462615967),\n",
       " ('gore', 0.4380093812942505),\n",
       " ('thrilling', 0.43581557273864746),\n",
       " ('thrills', 0.43550166487693787),\n",
       " ('explosions', 0.43038100004196167)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"action\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "540bf4ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8530463"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('france', 'germany')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "976ef522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04457998,  0.0307649 , -0.0174332 ,  0.06117793,  0.02018438,\n",
       "       -0.12031978, -0.02739874,  0.10649881, -0.00732701,  0.04741612,\n",
       "       -0.01702793,  0.03092411,  0.02579388,  0.03395401,  0.02595449,\n",
       "       -0.04911796, -0.01293617, -0.05300761, -0.03887616,  0.01161817,\n",
       "       -0.08139922, -0.06123571,  0.10276125,  0.00869878,  0.08679716,\n",
       "       -0.00914037,  0.07053868, -0.06762625,  0.01005393, -0.07019533,\n",
       "        0.05286527, -0.08623346, -0.03382185, -0.02542276,  0.04313236,\n",
       "        0.02382032, -0.00313712,  0.02032855,  0.06043682,  0.07967199,\n",
       "       -0.07651638,  0.04060363,  0.08804101, -0.05183306, -0.00748405,\n",
       "        0.05869152, -0.05898987,  0.02436483, -0.1426173 , -0.00806292,\n",
       "       -0.01102615, -0.00145902, -0.04552931,  0.012781  ,  0.05655881,\n",
       "        0.08123118,  0.06116894, -0.00588782,  0.03929471,  0.06501327,\n",
       "       -0.06296386, -0.00251321,  0.0286968 , -0.01467166,  0.05320358,\n",
       "        0.03404943, -0.08936688,  0.02383353, -0.06103896, -0.04121192,\n",
       "       -0.04390757,  0.0765492 ,  0.10894103,  0.01154363,  0.00556486,\n",
       "        0.00729677,  0.0402748 ,  0.02370943, -0.0030331 ,  0.06217123,\n",
       "        0.04080356, -0.10953636, -0.02107205,  0.10957135,  0.0328631 ,\n",
       "       -0.01990339, -0.0811128 , -0.00607169,  0.04634519,  0.01293297,\n",
       "        0.00656209,  0.04130935, -0.01358222,  0.0736469 ,  0.04186808,\n",
       "        0.01374981,  0.03079512,  0.00021761, -0.08813275,  0.00836335,\n",
       "        0.00211415,  0.00166518,  0.03380451, -0.05061782,  0.08781826,\n",
       "        0.01211573, -0.08210996,  0.0979584 , -0.04091812,  0.05145602,\n",
       "        0.03302958, -0.05563498,  0.06197524, -0.01054306,  0.12420541,\n",
       "       -0.04917355,  0.06821916,  0.009549  ,  0.00796722, -0.03757508,\n",
       "        0.0030985 ,  0.00744084,  0.01203547,  0.01002535, -0.11653987,\n",
       "        0.01575448,  0.00162362, -0.03096045, -0.05485077,  0.04016328,\n",
       "        0.01425265,  0.07384712, -0.0374896 , -0.04167664,  0.0854506 ,\n",
       "        0.06040336,  0.00766928,  0.00577534, -0.12336873, -0.01922933,\n",
       "       -0.00893782, -0.03539153,  0.01535479,  0.04561482,  0.01181792,\n",
       "       -0.02031184,  0.02580362, -0.05211001, -0.014391  ,  0.05632773,\n",
       "       -0.00901448, -0.11191246,  0.03056625, -0.03392352,  0.04505629,\n",
       "        0.01051526, -0.08947863, -0.08409009, -0.02537798,  0.14704216,\n",
       "        0.0400079 ,  0.02246647, -0.11289784,  0.19838044,  0.00059797,\n",
       "       -0.02058491,  0.11507108,  0.00262532,  0.068754  ,  0.12452362,\n",
       "       -0.04753044, -0.01564706,  0.02069695,  0.02375936, -0.0945247 ,\n",
       "        0.01572616, -0.02881064, -0.05862982, -0.0012831 ,  0.02283438,\n",
       "       -0.07593444, -0.0389055 , -0.03191274, -0.06923869, -0.02476503,\n",
       "        0.02266307, -0.01655379,  0.07103465, -0.03156516, -0.06753669,\n",
       "       -0.00355661, -0.01607314, -0.06604795,  0.03897255,  0.07212212,\n",
       "        0.00674791, -0.04398027,  0.02666869,  0.00295952, -0.04426964,\n",
       "       -0.06367013,  0.01381853,  0.04115697, -0.12236761, -0.04254464,\n",
       "        0.01044709, -0.00777812, -0.01475719, -0.06643764, -0.02297285,\n",
       "        0.02564482, -0.08289601, -0.03481232,  0.01012145,  0.08531585,\n",
       "       -0.0851147 , -0.01947078, -0.14391333, -0.11960632, -0.05780834,\n",
       "        0.06954271, -0.01924887, -0.07770381,  0.01437702, -0.12642749,\n",
       "       -0.02449627,  0.05842459, -0.05362117,  0.03211574, -0.00170736,\n",
       "       -0.00158874, -0.10635698, -0.1254199 , -0.04343018,  0.00891108,\n",
       "        0.037871  ,  0.00326119,  0.04601867, -0.00768057, -0.10931359,\n",
       "        0.05398136,  0.00887109, -0.13386384,  0.01987199, -0.03256411,\n",
       "       -0.05219338,  0.008083  , -0.0025282 , -0.01970085, -0.05574214,\n",
       "        0.00818194,  0.06412723,  0.00456237, -0.06831466, -0.00341563,\n",
       "       -0.04838158, -0.0013053 ,  0.01813871, -0.10388391,  0.03077155,\n",
       "        0.00088826, -0.02379256,  0.06077856, -0.09526428, -0.02840949,\n",
       "        0.00513209,  0.08173108,  0.08760326,  0.02139435,  0.05485644,\n",
       "       -0.01350745,  0.04160007, -0.04047093, -0.07879138,  0.11949006,\n",
       "       -0.04915085,  0.01167846, -0.06101599, -0.08269976, -0.03142648,\n",
       "        0.06462777, -0.01006877, -0.09188337, -0.06297687,  0.00227089,\n",
       "       -0.07034037, -0.11819853,  0.08854906,  0.08238342,  0.06321799,\n",
       "       -0.00694202,  0.06157761,  0.00360469, -0.06370911,  0.0255034 ,\n",
       "        0.10249633, -0.07707275, -0.07523321, -0.07210171,  0.00747466],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[\"flower\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b319aa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
